{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61364536-2376-402f-b31b-2c6c3618ffaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /opt/app-root/lib/python3.9/site-packages (1.15.0)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from onnx) (1.24.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/app-root/lib/python3.9/site-packages (from onnx) (4.25.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: onnxscript in /opt/app-root/lib/python3.9/site-packages (0.1.0.dev20240405)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from onnxscript) (4.10.0)\n",
      "Requirement already satisfied: onnx>=1.15 in /opt/app-root/lib/python3.9/site-packages (from onnxscript) (1.15.0)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from onnxscript) (1.24.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/app-root/lib/python3.9/site-packages (from onnx>=1.15->onnxscript) (4.25.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## 1. Install the required dependencies\n",
    "\n",
    "!pip install onnx\n",
    "!pip install onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014d1dc5-d9cc-4609-8f2e-2c9e96928fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## 2. Use existing model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        # Load the pre-trained weights\n",
    "        self.load_pretrained_weights()\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "        # Load the existing model weights\n",
    "        model_path = \"./best_metric_model.pth\"\n",
    "        pretrained_dict = torch.load(model_path)\n",
    "\n",
    "        # Initialize the model's state dictionary with pre-trained weights\n",
    "        model_dict = self.state_dict()\n",
    "\n",
    "        # Filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "\n",
    "        # Update the model's state dictionary\n",
    "        model_dict.update(pretrained_dict)\n",
    "\n",
    "        # Load the updated state dictionary into the model\n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of MyModel\n",
    "model = MyModel()\n",
    "\n",
    "# Optionally, you can print the model to verify if the weights are loaded correctly\n",
    "print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c93a49-a885-4882-ab3d-c91d705bda07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input.1 : Float(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu),\n",
      "      %conv1.weight : Float(6, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(6, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.weight : Float(16, 6, 5, 5, strides=[150, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(16, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc1.weight : Float(120, 400, strides=[400, 1], requires_grad=1, device=cpu),\n",
      "      %fc1.bias : Float(120, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc2.weight : Float(84, 120, strides=[120, 1], requires_grad=1, device=cpu),\n",
      "      %fc2.bias : Float(84, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc3.weight : Float(10, 84, strides=[84, 1], requires_grad=1, device=cpu),\n",
      "      %fc3.bias : Float(10, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/conv1/Conv_output_0 : Float(1, 6, 28, 28, strides=[4704, 784, 28, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv1/Conv\"](%input.1, %conv1.weight, %conv1.bias), scope: __main__.MyModel::/torch.nn.modules.conv.Conv2d::conv1 # /opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/Relu_output_0 : Float(1, 6, 28, 28, strides=[4704, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu\"](%/conv1/Conv_output_0), scope: __main__.MyModel:: # /opt/app-root/lib64/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %/MaxPool_output_0 : Float(1, 6, 14, 14, strides=[1176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/MaxPool\"](%/Relu_output_0), scope: __main__.MyModel:: # /opt/app-root/lib64/python3.9/site-packages/torch/nn/functional.py:782:0\n",
      "  %/conv2/Conv_output_0 : Float(1, 16, 10, 10, strides=[1600, 100, 10, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv2/Conv\"](%/MaxPool_output_0, %conv2.weight, %conv2.bias), scope: __main__.MyModel::/torch.nn.modules.conv.Conv2d::conv2 # /opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/Relu_1_output_0 : Float(1, 16, 10, 10, strides=[1600, 100, 10, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu_1\"](%/conv2/Conv_output_0), scope: __main__.MyModel:: # /opt/app-root/lib64/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %/MaxPool_1_output_0 : Float(1, 16, 5, 5, strides=[400, 25, 5, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/MaxPool_1\"](%/Relu_1_output_0), scope: __main__.MyModel:: # /opt/app-root/lib64/python3.9/site-packages/torch/nn/functional.py:782:0\n",
      "  %/Flatten_output_0 : Float(1, 400, strides=[400, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/Flatten\"](%/MaxPool_1_output_0), scope: __main__.MyModel:: # /tmp/ipykernel_5965/1499645524.py:39:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 120, strides=[120, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Flatten_output_0, %fc1.weight, %fc1.bias), scope: __main__.MyModel::/torch.nn.modules.linear.Linear::fc1 # /opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_2_output_0 : Float(1, 120, strides=[120, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu_2\"](%/fc1/Gemm_output_0), scope: __main__.MyModel:: # /opt/app-root/lib64/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 84, strides=[84, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Relu_2_output_0, %fc2.weight, %fc2.bias), scope: __main__.MyModel::/torch.nn.modules.linear.Linear::fc2 # /opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_3_output_0 : Float(1, 84, strides=[84, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu_3\"](%/fc2/Gemm_output_0), scope: __main__.MyModel:: # /opt/app-root/lib64/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %22 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Relu_3_output_0, %fc3.weight, %fc3.bias), scope: __main__.MyModel::/torch.nn.modules.linear.Linear::fc3 # /opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%22)\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 3.Export the model to ONNX format\n",
    "\n",
    "torch_input = torch.randn(1, 1, 32, 32)\n",
    "output_path = \"my_model.onnx\"\n",
    "torch.onnx.export(model, torch_input, output_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab6a845-a805-4e04-a702-f90bd853a476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved as spl.onnx\n"
     ]
    }
   ],
   "source": [
    "## 4. Save the ONNX model in a file\n",
    "import os\n",
    "\n",
    "# Rename the ONNX file if needed\n",
    "output_path = \"spl.onnx\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"my_model.onnx\"):\n",
    "    # Rename the file\n",
    "    os.rename(\"my_model.onnx\", output_path)\n",
    "    print(f\"ONNX model saved as {output_path}\")\n",
    "else:\n",
    "    print(\"Error: The ONNX file does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60122f5-9227-4185-a37d-a97f8f25765e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load the ONNX file back into memory and check if it is well formed with the following code:\n",
    "\n",
    "\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"spl.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b95fbfef-7de9-41c4-8e2e-f03e3cd582cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: [(1, 10)]\n"
     ]
    }
   ],
   "source": [
    "## 6. Execute the ONNX model with ONNX Runtime\n",
    "\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `torch_input` is a torch.Tensor\n",
    "torch_input = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "# Assuming `onnx_model` is already loaded\n",
    "onnx_model = onnx.load(\"spl.onnx\")\n",
    "\n",
    "# Create ONNX runtime session\n",
    "ort_session = onnxruntime.InferenceSession(\"spl.onnx\")\n",
    "\n",
    "# Adapt torch inputs to ONNX inputs\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "onnx_input = {input_name: torch_input.numpy()}  # Convert torch tensor to numpy array\n",
    "\n",
    "# Run inference\n",
    "onnxruntime_outputs = ort_session.run(None, onnx_input)\n",
    "\n",
    "# Print results\n",
    "print(\"Output shape:\", [output.shape for output in onnxruntime_outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11b8d2d-6126-48a9-959b-db3ba35a41c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch output: tensor([[ 0.0562,  0.1270,  0.0226, -0.0884, -0.0753,  0.0391,  0.0653, -0.0704,\n",
      "          0.1106, -0.0061]], grad_fn=<ReshapeAliasBackward0>)\n",
      "ONNX Runtime output: tensor([[-0.0338, -0.0471,  0.0242,  0.0872,  0.1257,  0.1178,  0.0555,  0.1461,\n",
      "          0.0130, -0.0281]])\n",
      "Difference found at index (0, 0):\n",
      "PyTorch value: 0.056172024458646774\n",
      "ONNX Runtime value: -0.03375621885061264\n",
      "Difference found at index (0, 1):\n",
      "PyTorch value: 0.12696225941181183\n",
      "ONNX Runtime value: -0.04705527424812317\n",
      "Difference found at index (0, 2):\n",
      "PyTorch value: 0.022590460255742073\n",
      "ONNX Runtime value: 0.024227982386946678\n",
      "Difference found at index (0, 3):\n",
      "PyTorch value: -0.08837180584669113\n",
      "ONNX Runtime value: 0.08721882104873657\n",
      "Difference found at index (0, 4):\n",
      "PyTorch value: -0.07527130097150803\n",
      "ONNX Runtime value: 0.12569761276245117\n",
      "Difference found at index (0, 5):\n",
      "PyTorch value: 0.03910406678915024\n",
      "ONNX Runtime value: 0.11777838319540024\n",
      "Difference found at index (0, 6):\n",
      "PyTorch value: 0.06534945219755173\n",
      "ONNX Runtime value: 0.05554559826850891\n",
      "Difference found at index (0, 7):\n",
      "PyTorch value: -0.07037550956010818\n",
      "ONNX Runtime value: 0.14612719416618347\n",
      "Difference found at index (0, 8):\n",
      "PyTorch value: 0.11055848002433777\n",
      "ONNX Runtime value: 0.013034806586802006\n",
      "Difference found at index (0, 9):\n",
      "PyTorch value: -0.006144792772829533\n",
      "ONNX Runtime value: -0.028063742443919182\n",
      "PyTorch and ONNX Runtime output matched!\n",
      "Output length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5965/2041154309.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  onnx_val_tensor = torch.tensor(onnxruntime_output).detach()  # Cloning and detaching the tensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `torch_input` is a torch.Tensor\n",
    "torch_input = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "# Assuming `torch_model` is your PyTorch model\n",
    "torch_model = MyModel()\n",
    "\n",
    "# Run inference with PyTorch model\n",
    "torch_outputs = torch_model(torch_input)\n",
    "\n",
    "# Create ONNX runtime session\n",
    "ort_session = onnxruntime.InferenceSession(\"spl.onnx\")\n",
    "\n",
    "# Adapt torch outputs to ONNX outputs (not required)\n",
    "# No need to adapt torch outputs, they can be directly compared with ONNX outputs\n",
    "\n",
    "# Run inference with ONNX runtime\n",
    "onnxruntime_inputs = {ort_session.get_inputs()[0].name: torch_input.detach().numpy()}\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_inputs)\n",
    "\n",
    "# Convert ONNX runtime outputs to torch tensors for comparison\n",
    "onnxruntime_outputs = [torch.tensor(output) for output in onnxruntime_outputs]\n",
    "\n",
    "\n",
    "assert len(torch_outputs) == len(onnxruntime_outputs)\n",
    "for torch_output, onnxruntime_output in zip(torch_outputs, onnxruntime_outputs):\n",
    "    # Ignore batch dimension mismatch if present\n",
    "    if torch_output.shape[0] == 1:\n",
    "        torch_output = torch_output.squeeze(0)\n",
    "    \n",
    "    # Reshape PyTorch output to match ONNX Runtime output shape\n",
    "    torch_output = torch_output.reshape(onnxruntime_output.shape)\n",
    "\n",
    "    # Print values of both outputs for comparison\n",
    "    print(\"PyTorch output:\", torch_output)\n",
    "    print(\"ONNX Runtime output:\", onnxruntime_output)\n",
    "\n",
    "    # Convert numpy array back to PyTorch tensor\n",
    "    onnx_val_tensor = torch.tensor(onnxruntime_output).detach()  # Cloning and detaching the tensor\n",
    "\n",
    "    # Check for element-wise closeness\n",
    "    for index, torch_val in np.ndenumerate(torch_output.detach().numpy()):\n",
    "        onnx_val = onnx_val_tensor[index]  # Access the corresponding value from ONNX Runtime output\n",
    "        if not torch.isclose(torch.tensor(torch_val), onnx_val, atol=1e-05, rtol=1e-06):\n",
    "            print(f\"Difference found at index {index}:\")\n",
    "            print(f\"PyTorch value: {torch_val}\")\n",
    "            print(f\"ONNX Runtime value: {onnx_val}\")\n",
    "\n",
    "print(\"PyTorch and ONNX Runtime output matched!\")\n",
    "print(f\"Output length: {len(onnxruntime_outputs)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ad5e8-1467-4810-aaea-f1b8cbf974c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
